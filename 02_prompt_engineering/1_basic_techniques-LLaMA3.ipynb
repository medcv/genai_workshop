{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prompt Engineering Basic Techniques\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio on ml.t3.medium instance*\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the boto3 Python SDK to work with Amazon Bedrock Foundation Models. If you are running this in AWS provided accounts, excessive API calls to Bedrock APIs may results in throttling and your account may get blocked\n",
    "\n",
    "In this demo notebook, we are going to explore some of the PROMPT Engineering Techniques for better model output generation.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7dadd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating base environment\n",
      "awscli>=1.29.57 has been installed successfully.\n",
      "pydantic>2 has been installed successfully.\n",
      "Base environment validated successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #4cc9f0; text-decoration-color: #4cc9f0; font-weight: bold\">Validating lab environment from requirements.txt</span> ‚ú®\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;76;201;240mValidating lab environment from requirements.txt\u001b[0m ‚ú®\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #e85d04; text-decoration-color: #e85d04; font-weight: bold; text-decoration: underline\">ENVIRONMENT STATUS</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> langchain-aws is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> sqlalchemy is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> datasets is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> matplotlib is installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">langchain</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\">==</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">0.1</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\">.</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">14</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\"> is not installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">pypdf&gt;=</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">3.8</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\">,&lt;</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">4</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\"> is not installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">pymupdf  is not installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">xmltodict</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\">==</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">0.13</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\">.</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">0</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\"> is not installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">duckduckgo-search is not installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">yfinance is not installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">pandas-datareader is not installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">pysqlite3 is not installed</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;4;38;2;232;93;4mENVIRONMENT STATUS\u001b[0m\n",
       "‚úÖ \u001b[32m langchain-aws is installed\u001b[0m\n",
       "‚úÖ \u001b[32m sqlalchemy is installed\u001b[0m\n",
       "‚úÖ \u001b[32m datasets is installed\u001b[0m\n",
       "‚úÖ \u001b[32m matplotlib is installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mlangchain\u001b[0m\u001b[38;2;239;35;60m==\u001b[0m\u001b[1;38;2;239;35;60m0.1\u001b[0m\u001b[38;2;239;35;60m.\u001b[0m\u001b[1;38;2;239;35;60m14\u001b[0m\u001b[38;2;239;35;60m is not installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mpypdf>=\u001b[0m\u001b[1;38;2;239;35;60m3.8\u001b[0m\u001b[38;2;239;35;60m,<\u001b[0m\u001b[1;38;2;239;35;60m4\u001b[0m\u001b[38;2;239;35;60m is not installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mpymupdf  is not installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mxmltodict\u001b[0m\u001b[38;2;239;35;60m==\u001b[0m\u001b[1;38;2;239;35;60m0.13\u001b[0m\u001b[38;2;239;35;60m.\u001b[0m\u001b[1;38;2;239;35;60m0\u001b[0m\u001b[38;2;239;35;60m is not installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mduckduckgo-search is not installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60myfinance is not installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mpandas-datareader is not installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mpysqlite3 is not installed\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Installing missing libraries</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mInstalling missing libraries\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==0.1.14 has been installed successfully.\n",
      "pypdf>=3.8,<4 has been installed successfully.\n",
      "pymupdf  has been installed successfully.\n",
      "xmltodict==0.13.0 has been installed successfully.\n",
      "duckduckgo-search has been installed successfully.\n",
      "yfinance has been installed successfully.\n",
      "pandas-datareader has been installed successfully.\n",
      "pysqlite3 has been installed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #a7c957; text-decoration-color: #a7c957\">All required libraries are installed.üéâ</span>\n",
       "<span style=\"color: #a7c957; text-decoration-color: #a7c957\">You may proceed with the lab! üöÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;167;201;87mAll required libraries are installed.üéâ\u001b[0m\n",
       "\u001b[38;2;167;201;87mYou may proceed with the lab! üöÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d044f158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #e85d04; text-decoration-color: #e85d04; font-weight: bold; text-decoration: underline\">MODEL ACCESS STATUS</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> amazon.titan-embed-text-v1 is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> anthropic.claude-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\">-sonnet-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">20240229</span><span style=\"color: #008000; text-decoration-color: #008000\">-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1:0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> anthropic.claude-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span><span style=\"color: #008000; text-decoration-color: #008000\">-haiku-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">20240307</span><span style=\"color: #008000; text-decoration-color: #008000\">-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1:0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> meta.llama3-70b-instruct-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1:0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> meta.llama3-8b-instruct-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1:0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> mistral.mixtral-8x7b-instruct-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0:1</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;4;38;2;232;93;4mMODEL ACCESS STATUS\u001b[0m\n",
       "‚úÖ \u001b[32m amazon.titan-embed-text-v1 is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m anthropic.claude-\u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m-sonnet-\u001b[0m\u001b[1;32m20240229\u001b[0m\u001b[32m-v\u001b[0m\u001b[1;32m1:0\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m anthropic.claude-\u001b[0m\u001b[1;32m3\u001b[0m\u001b[32m-haiku-\u001b[0m\u001b[1;32m20240307\u001b[0m\u001b[32m-v\u001b[0m\u001b[1;32m1:0\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m meta.llama3-70b-instruct-v\u001b[0m\u001b[1;32m1:0\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m meta.llama3-8b-instruct-v\u001b[0m\u001b[1;32m1:0\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m mistral.mixtral-8x7b-instruct-v\u001b[0m\u001b[1;32m0:1\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #a7c957; text-decoration-color: #a7c957\">All required models are accessible.üéâ</span>\n",
       "<span style=\"color: #a7c957; text-decoration-color: #a7c957\">You may proceed with the lab! üöÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;167;201;87mAll required models are accessible.üéâ\u001b[0m\n",
       "\u001b[38;2;167;201;87mYou may proceed with the lab! üöÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v1\",\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"meta.llama3-70b-instruct-v1:0\",\n",
    "    \"meta.llama3-8b-instruct-v1:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "]\n",
    "\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create the bedrock client for model access\n",
    "import json\n",
    "\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from utils import bedrock, print_ww\n",
    "from utils.prompt_utils import prompts_to_messages, convert_pdf_to_image, convert_pil_image_to_b64\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9174c4-326a-463e-92e1-8c7e47111269",
   "metadata": {},
   "source": [
    "#### Validate the connection\n",
    "\n",
    "We can check the client works by trying out the `list_foundation_models()` method, which will tell us all the models available for us to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67b4466-12ff-4975-9811-7a19c6206604",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'a9644b54-3bc1-4fd9-b1cf-5d0e52a3a684',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sun, 25 Aug 2024 14:22:00 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '28195',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'a9644b54-3bc1-4fd9-b1cf-5d0e52a3a684'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n",
       "   'modelId': 'amazon.titan-tg1-large',\n",
       "   'modelName': 'Titan Text Large',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v1:0',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1',\n",
       "   'modelId': 'amazon.titan-image-generator-v1',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v2:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v2:0',\n",
       "   'modelName': 'Titan Image Generator G1 v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED', 'ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-premier-v1:0',\n",
       "   'modelId': 'amazon.titan-text-premier-v1:0',\n",
       "   'modelName': 'Titan Text G1 - Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "   'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "   'modelName': 'Titan Text Embeddings v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1',\n",
       "   'modelId': 'amazon.titan-text-lite-v1',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "   'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1',\n",
       "   'modelId': 'amazon.titan-text-express-v1',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1',\n",
       "   'modelId': 'amazon.titan-embed-text-v1',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "   'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1',\n",
       "   'modelId': 'amazon.titan-embed-image-v1',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1:0',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1:0',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-grande-instruct',\n",
       "   'modelId': 'ai21.j2-grande-instruct',\n",
       "   'modelName': 'J2 Grande Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-jumbo-instruct',\n",
       "   'modelId': 'ai21.j2-jumbo-instruct',\n",
       "   'modelName': 'J2 Jumbo Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid',\n",
       "   'modelId': 'ai21.j2-mid',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid-v1',\n",
       "   'modelId': 'ai21.j2-mid-v1',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra',\n",
       "   'modelId': 'ai21.j2-ultra',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1:0:8k',\n",
       "   'modelId': 'ai21.j2-ultra-v1:0:8k',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1',\n",
       "   'modelId': 'ai21.j2-ultra-v1',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.jamba-instruct-v1:0',\n",
       "   'modelId': 'ai21.jamba-instruct-v1:0',\n",
       "   'modelName': 'Jamba-Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k',\n",
       "   'modelId': 'anthropic.claude-instant-v1:2:100k',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:18k',\n",
       "   'modelId': 'anthropic.claude-v2:0:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:100k',\n",
       "   'modelId': 'anthropic.claude-v2:0:100k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:18k',\n",
       "   'modelId': 'anthropic.claude-v2:1:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:200k',\n",
       "   'modelId': 'anthropic.claude-v2:1:200k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1',\n",
       "   'modelId': 'anthropic.claude-v2:1',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2',\n",
       "   'modelId': 'anthropic.claude-v2',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "   'modelName': 'Claude 3.5 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-text-v14:7:4k',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14',\n",
       "   'modelId': 'cohere.command-text-v14',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-v1:0',\n",
       "   'modelId': 'cohere.command-r-v1:0',\n",
       "   'modelName': 'Command R',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-plus-v1:0',\n",
       "   'modelId': 'cohere.command-r-plus-v1:0',\n",
       "   'modelName': 'Command R+',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-light-text-v14:7:4k',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14',\n",
       "   'modelId': 'cohere.command-light-text-v14',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3:0:512',\n",
       "   'modelId': 'cohere.embed-english-v3:0:512',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3',\n",
       "   'modelId': 'cohere.embed-english-v3',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3:0:512',\n",
       "   'modelId': 'cohere.embed-multilingual-v3:0:512',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3',\n",
       "   'modelId': 'cohere.embed-multilingual-v3',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1',\n",
       "   'modelId': 'meta.llama2-13b-v1',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1',\n",
       "   'modelId': 'meta.llama2-70b-v1',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-8b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-8b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelId': 'mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelName': 'Mistral 7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelId': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelName': 'Mixtral 8x7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-large-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-large-2402-v1:0',\n",
       "   'modelName': 'Mistral Large (2402)',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-small-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-small-2402-v1:0',\n",
       "   'modelName': 'Mistral Small',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "boto3_bedrock.list_foundation_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f690043-df45-448f-8fa6-1ea8b06f1087",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## `InvokeModel` body and output\n",
    "\n",
    "The `invoke_model()` method of the Amazon Bedrock runtime client (`InvokeModel` API) will be the primary method we use for most of our Text Generation and Processing tasks - whichever model we're using.\n",
    "\n",
    "Although the method is shared, the format of input and output varies depending on the foundation model used - as described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb6bee-7654-4269-9127-9afa4e823454",
   "metadata": {},
   "source": [
    "### Anthropic Claude Completion API format\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"\\n\\nHuman:<prompt>\\n\\nAnswer:\",\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"completion\": \"<output>\",\n",
    "    \"stop_reason\": \"stop_sequence\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511af34-3a22-4af8-a1ad-8717c9d6a8a3",
   "metadata": {},
   "source": [
    "### Anthropic Claude Messages API format\n",
    "\n",
    "#### Example with a single user message: \n",
    "\n",
    "```json\n",
    "[{\"role\": \"user\", \"content\": \"Hello, Claude\"}]\n",
    "```\n",
    "\n",
    "#### Example with multiple conversational turns:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"role\": \"user\", \"content\": \"Hello there.\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\"},\n",
    "  {\"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "#### Example with a partially-filled response from Claude:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"role\": \"user\", \"content\": \"Please describe yourself using only JSON\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Here is my JSON description:\\n{\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Each input message content may be either a single string or an array of content blocks, where each block has a specific type. Using a string is shorthand for an array of one content block of type \"text\". The following input messages are equivalent:\n",
    "\n",
    "```json\n",
    "{\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, Claude\"}]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22c308-ebbf-4ef5-a823-832b7c236e31",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Try out the Claude Model\n",
    "\n",
    "With some theory out of the way, let's see the models in action! Run the cells below to see basic, synchronous example invocations for each model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a0a79b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")\n",
    "\n",
    "modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "def invoke_model_and_get_response(prompt_data): \n",
    "\n",
    "    \n",
    "    \n",
    "    body = json.dumps({ \n",
    "        'prompt': prompt_data,\n",
    "        'max_gen_len': 1024,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.2\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "        response_body = json.loads(response.get('body').read().decode('utf-8'))\n",
    "        outputText = response_body['generation'].strip()\n",
    "        return outputText\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "            return (f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                    \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "                     \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                     \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d46f6-cc59-47eb-8442-dea03a8dd4e6",
   "metadata": {},
   "source": [
    "#### Claude Prompts\n",
    "As the message based API has become the standard way to prompt Claude models, we will utilize a small helper function `prompts_to_messages` that will convert our prompt into the message format expected by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a725de2-bdea-4d86-b12d-d1d7cdda010b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"Write me a blog about making strong business decisions as a leader\"\"\"\n",
    "\n",
    "prompt_data = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Hi Llama, how are you? [/INST]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ba33ac0-fa16-4c4f-b882-e838d0cb5830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[/SYS]\\n```\\n\\n**Llama's Response:**\\n\\nHello! I'm doing great, thanks for asking! I'm here to assist you with any questions or concerns you may have. How can I help you today? [/INST] [/SYS]\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "invoke_model_and_get_response(prompt_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prompt Engineering - Basic Techiques\n",
    "\n",
    "In this section, we are going to explore some basic prompting engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0ae11-c2cb-4c0c-b656-582703d1673b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Zero-shot prompting\n",
    "Zero Shot prompting describes the technique where we present a task to an LLM without giving it further examples. We therefore, expect it to perform the task without getting a prior ‚Äúshot‚Äù at the task. Hence, ‚Äúzero-shot‚Äù prompting. Modern LLMs demonstrate remarkable zero-shot performance and a positive correlation can be drawn between model size and zero-shot performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7079ffff-9356-44e6-9f4d-ee16d2f08964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "system_prompt = \"\"\"You are a customer service agent tasked with classifying emails by type. Please output your answer and then justify your classification. How would you categorize this email? \"\"\"\n",
    "\n",
    "prompt_data = \"\"\"\n",
    "<email>\n",
    "I would like to know how I can contribute to my retirement account? Any additional resources to read about IRA would be helpful\n",
    "</email> \n",
    "Provide and explanation for your choice of answer\n",
    "\n",
    "The categories are: \n",
    "(A) IRA \n",
    "(B) 529 Plan\n",
    "(C) Cash Management\n",
    "(D) Youth Account\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbda30c4-5498-4de3-b7bd-764819d3659b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on the content of the email, I would categorize it as:\n",
       "\n",
       "<span style=\"font-weight: bold\">(</span>A<span style=\"font-weight: bold\">)</span> IRA\n",
       "\n",
       "Justification:\n",
       "The email explicitly mentions <span style=\"color: #008000; text-decoration-color: #008000\">\"retirement account\"</span> and asks about contributing to an IRA <span style=\"font-weight: bold\">(</span>Individual Retirement \n",
       "Account<span style=\"font-weight: bold\">)</span>. The customer is seeking information related to Individual Retirement Accounts, which are tax-advantaged \n",
       "accounts designed for retirement savings. The customer also requests additional resources to read about IRAs, \n",
       "further confirming that the inquiry is specifically about IRAs and retirement savings plans.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on the content of the email, I would categorize it as:\n",
       "\n",
       "\u001b[1m(\u001b[0mA\u001b[1m)\u001b[0m IRA\n",
       "\n",
       "Justification:\n",
       "The email explicitly mentions \u001b[32m\"retirement account\"\u001b[0m and asks about contributing to an IRA \u001b[1m(\u001b[0mIndividual Retirement \n",
       "Account\u001b[1m)\u001b[0m. The customer is seeking information related to Individual Retirement Accounts, which are tax-advantaged \n",
       "accounts designed for retirement savings. The customer also requests additional resources to read about IRAs, \n",
       "further confirming that the inquiry is specifically about IRAs and retirement savings plans.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"system\": system_prompt,\n",
    "    \"messages\": prompts_to_messages(prompt_data),\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "rprint(response_body.get(\"content\")[0][\"text\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe215b9-1482-4e0e-a3d4-56880393a3bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Few-shot prompting\n",
    "Giving the model more information about the tasks at hand via examples is called Few-Shot Prompting. It can be used for in-context learning by providing examples of the task and the desired output. We can therefore condition the model on the examples to follow the task guidance more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72b95a-5855-4699-a499-23cc6bb9ea10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"Your job is to generate financial summaries similar to the examples below: \n",
    "\n",
    "Microsoft: Microsoft reported record revenue of $51.9 billion in its latest quarterly earnings report, a 20% increase year-over-year. Its net income rose to $16.7 billion, up 21% compared to the previous year. Microsoft's revenue growth was driven by its cloud computing business Azure and Office 365 productivity tools.\n",
    "\n",
    "Amazon: In its most recent quarterly results, Amazon reported net sales of $96.1 billion, an increase of 15% compared to the same period last year. Amazon's net income decreased to $2.9 billion, down 58% year-over-year, as the company faced rising costs due to inflation and supply chain issues. Amazon Web Services, its cloud computing segment, continued to see strong growth with sales up 33%.\n",
    "\n",
    "Now provide a brief financial summary for Apple Inc. based on the following key facts:\n",
    "- Revenue for the last 12 months: $260 billion \n",
    "- Net income: $55 billion\n",
    "- Total assets: $321 billion\n",
    "- Market capitalization: $2.2 trillion\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bee5a-bcf4-4b06-b2ab-68c29b82b2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "body = json.dumps(\n",
    "    {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"messages\": prompts_to_messages(prompt_data),\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    }\n",
    ")\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "rprint(response_body.get(\"content\")[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b831f3a-0583-4b97-acfa-1379acd74213",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Chain of Thought prompting\n",
    "Chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding. The main idea of CoT is that by showing the LLM some few shot exemplars where the reasoning process is explained in the exemplars, the LLM will also show the reasoning process when answering the prompt. This explanation of reasoning often leads to more accurate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba3bec-9053-46a7-a526-83919153b16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "prompt_data = \"\"\"\n",
    "Apple Inc. key facts:\n",
    "- Revenue for the last 12 months: $260 billion \n",
    "- Operating Expenses: $205 billion\n",
    "- Total assets: $321 billion\n",
    "- Market capitalization: $2.2 trillion\n",
    "\n",
    "If Apple's revenue is expected to grow by 10% in the next year, however it's expenses are expected to increase by 15%, what would be the expected net income for Apple in the next year?\n",
    "\n",
    "Think step-by-step and provide your thought process in the <thinking></thinking> XML tags and the final answer in the <answer></answer> XML tags.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d1929-8950-4937-a005-546bb8b2b715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = json.dumps(\n",
    "    {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"messages\": prompts_to_messages(prompt_data),\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    }\n",
    ")\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body.get(\"content\")[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf24ea7-7195-4286-8782-85d776c3673e",
   "metadata": {},
   "source": [
    "## Prompt Engineering - Different task examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc4596-d76e-4abe-a77e-0259db5ed14e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Conversation Summarization\n",
    "Prompt engineering for conversation summarization is a process of crafting prompts that guide a language model like Claude in summarizing dialogues or conversations effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611d319-f3a6-4705-a84c-11a666b41324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation_summary_prompt = \"\"\"\n",
    "Provide a concise bullet-point summary of the following conversation\n",
    "<conversation>\n",
    "Susan: Welcome back, Chuck. This is the second part of our interview, and I want to ask you these questions as quickly as possible because I know you have a flight to catch, so. . . .\n",
    "\n",
    "Chuck: No problem. I‚Äôm happy to chat with you.\n",
    "\n",
    "Susan: It says on your website that you never studied business before accepting a marketing role at a Spanish-speaking multimedia publisher. Was ‚Ä¶ was that‚Äî\n",
    "\n",
    "Chuck: Yeah.\n",
    "\n",
    "Susan: So was that more of a stretch, or ‚Ä¶ ?\n",
    "\n",
    "Chuck: Yeah, well, actua‚Äî\n",
    "\n",
    "Susan: Oh, I‚Äôm sorry, just a minute. Can everyone else just put themselves on mute, please?\n",
    "\n",
    "Akim: [inaudible 00:37]\n",
    "\n",
    "Susan: Great, thanks. OK, so where were we? So, I was asking you if, when you took the new role as Director, it felt like a stretch?\n",
    "\n",
    "Chuck: Right, yeah. . . . I knew there would be a period of adjustment that I just would have to push through, you know?\n",
    "\n",
    "Susan: Absolutely. Were there any other challenges or roadblocks that you weren‚Äôt expecting though?\n",
    "\n",
    "Chuck: Hm, roadblocks I wasn‚Äôt expecting. Um‚Ä¶\n",
    "\n",
    "Susan: [laughs] I keep throwing you curveballs.\n",
    "\n",
    "Chuck: No, it‚Äôs OK. So, I actually didn‚Äôt anticipate the workload involved in learning a foreign language on the job. It‚Äôs like, you know, sometimes. . . . I mean, you need to be really flexible and ready to change it up if your strategy isn‚Äôt actually working. Know what I mean?\n",
    "\n",
    "Susan: [laughs] Definitely. So what was your budget like, I mean, was that, like, a challenge, too?\n",
    "\n",
    "Chuck: Hoo boy. [laughs] Yeah it definitely was. We had funding available, but we just, um, needed so many hands on deck. It was hard to, you know, actually manage so many people with our existing resources. [coughs]\n",
    "\n",
    "Susan: I totally understand. So what was your next move? I mean, did you‚Äî\n",
    "\n",
    "Chuck: Yeah, yeah. . . . So next, I kind of wanted to see where I do a gap analysis and throw resources at new initiatives where we weren‚Äôt, uh, I mean, I wanted to actually close the loop in those places where we didn‚Äôt quite have a foothold.\n",
    "\n",
    "Susan: Nice, nice.\n",
    "\n",
    "Chuck: Yeah, we really just needed to keep what was working and then lay out a new plan for [clears throat] future growth.\n",
    "\n",
    "Susan: So, it looks like we have time for maybe, um, let‚Äôs see‚Äî maybe one more question.\n",
    "\n",
    "Chuck: Sounds good. Shoot.\n",
    "\n",
    "Susan: How did this experience change the way you approached new challenges going forward?\n",
    "\n",
    "Chuck: Hm, good one. [laughs] I think that I was less, uh, likely to question my own judgement when it came to adaptability. Now I know that I really have it in me, you know?\n",
    "\n",
    "Really all it takes is a great marketing team underneath you and the freelance resources to put together the best marketing initiative you can afford, you know, with, um, the standard tools high-quality content, good ad placement, good SEO, a strong social presence and then maybe debut a new idea every six months. Um, like, you could come up with a certain theme or catch phrase and actually weave it through all your branding and regular initiatives.\n",
    "\n",
    "Susan: OK, well thanks, Chuck. This is really good stuff. I know you have to run, um, but I‚Äôd just like to thank you for, you know, taking this time with me today.\n",
    "\n",
    "Chuck: Sure, sure. Of course\n",
    "</conversation>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dd02c-3900-489f-95f7-3c7ac1bae9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": prompts_to_messages(conversation_summary_prompt),\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "rprint(response_body.get(\"content\")[0][\"text\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c65925-1a32-4d05-971e-74e2778d4b0c",
   "metadata": {},
   "source": [
    "### Question & Answering\n",
    "Prompt engineering for question answering is a process of crafting prompts that guide a language model like Claude in answering questions on provided context effectively. This is used as a basis for more advanced techniques such as Retrieval-Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c60cfc-118b-4025-afd7-9585acbb2221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a customer service agent tasked with answering questions about various financial products and services. Please output your answer and then justify your classification. \"\"\"\n",
    "\n",
    "qna_prompt = \"\"\"\n",
    "<context>\n",
    "When the teen turns 18, their account is eligible to transition to the regular Fidelity brokerage account with expanded features like option and margin trading.\n",
    "The teen will be prompted to transition their account starting on their 18th birthday. They will have 60 days to do so before their debit card and ability to trade will be restricted. Once the account is transitioned, the debit card that the teen was issued for their Fidelity Youth‚Ñ¢ Account will continue to be valid until it expires; at that point, a new debit card will be issued.\n",
    "Once a teen turns 18, the teen may choose whether or not the parent/guardian will continue to have access to that teen‚Äôs Fidelity Youth Account information. Teens can still use the Fidelity Youth‚Ñ¢ app when they turn 18, however additional capabilities are available to them in the Fidelity mobile app.\n",
    "</context> \n",
    "\n",
    "<question> What is the role of a parent/ guardian according to Fidelity youth accounts? </question>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112947e-dd93-416d-8578-67cc4cc06656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = json.dumps(\n",
    "    {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"system\": system_prompt,\n",
    "        \"messages\": prompts_to_messages(qna_prompt),\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    }\n",
    ")\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"  \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "rprint(response_body.get(\"content\")[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c75d1-ea68-4db0-b1fe-3f195c5206f9",
   "metadata": {},
   "source": [
    "### Content Generation\n",
    "We can use prompt engineering to guide the model in generating content that is relevant to the task at hand. This can be used for generating content for a variety of tasks such as content creation, content summarization, and more.\n",
    "In this example, the model will generate a table using provided data and as well as some questions that it can answer based on this data. This approach is common for getting the model to address more complex research type tasks where multiple steps are involved in generating the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c2ebf-b9a8-473b-8c1a-16daec656857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_prompt = \"\"\"Below is research that you performed previously. On basis of this research please answer the question below. \n",
    "If it does not include the exact answer, please respond with relevant information from the research in the response. Prefer more recent sources when possible.\n",
    "If the context contains publication ids, please include citations in the response. Citations should be formatted as [publication_id] or [org_id]\n",
    "If the user asks for a chart, return only machine readable json structure for the chart in HighCharts format without any formatting. \n",
    "Prefer answering in First Person (I, me, my) style.\n",
    "In your response, after you answer draw a line, and include 3 additional questions you can answer with the research, one question per line.\n",
    "Example:\n",
    "{Your Answer}\n",
    "\n",
    "Additional questions I can answer::\n",
    "Question 1\n",
    "Question 2\n",
    "Question 3\n",
    "\n",
    "<research>\n",
    "JPMorgan Chase Bank, N.A. (164500) Financials: \n",
    " \n",
    "|Indicator|2020(annual)|2021(annual)|2022(annual)|\n",
    "|---|---|---|---|\n",
    "|Total assets (USD Billion)|3025.28|3306.98|3201.94|\n",
    "|Tangible Common Equity (USD Billion)|221.41|262.33|280.96|\n",
    "|Problem Loans / Gross Loans|1.64|1.14|1.00|\n",
    "|Tangible Common Equity / Risk Weighted Assets|16.48|18.83|19.04|\n",
    "|Problem Loans / (Tangible Common Equity + Loan Loss Reserve)|6.71|4.44|3.80|\n",
    "|Net Interest Margin|2.13|1.74|2.22|\n",
    "|PPI / Average RWA|3.30|2.90|3.45|\n",
    "|Net Income / Tangible Assets|0.70|1.16|1.09|\n",
    "|Cost / Income Ratio|58.76|61.90|57.71|\n",
    "|Market Funds / Tangible Banking Assets|15.38|12.38|12.43|\n",
    "|Liquid Banking Assets / Tangible Banking Assets|55.38|55.31|49.91|\n",
    "|Gross Loans / Due to Customers|47.24|44.01|48.72|\n",
    " \n",
    "Bank of America Corporation (541000) Financials: \n",
    " \n",
    "|Indicator|2020(annual)|2021(annual)|2022(annual)|\n",
    "|---|---|---|---|\n",
    "|Total assets (USD Billion)|2786.01|3146.17|3024.42|\n",
    "|Tangible Common Equity (USD Billion)|174.68|175.58|190.40|\n",
    "|Problem Loans / Gross Loans|1.18|0.95|0.91|\n",
    "|Tangible Common Equity / Risk Weighted Assets|12.74|12.55|13.49|\n",
    "|Problem Loans / (Tangible Common Equity + Loan Loss Reserve)|5.92|5.26|4.87|\n",
    "|Net Interest Margin|1.75|1.52|1.82|\n",
    "|PPI / Average RWA|1.94|1.96|2.23|\n",
    "|Net Income / Tangible Assets|0.67|0.92|0.71|\n",
    "|Cost / Income Ratio|67.23|69.57|66.12|\n",
    "|Market Funds / Tangible Banking Assets|21.09|21.26|22.71|\n",
    "|Liquid Banking Assets / Tangible Banking Assets|57.50|58.98|55.10|\n",
    "|Gross Loans / Due to Customers|54.20|50.35|56.05|\n",
    " \n",
    "Wells Fargo Bank, N.A. (811500) Financials: \n",
    " \n",
    "|Indicator|2020(annual)|2021(annual)|2022(annual)|\n",
    "|---|---|---|---|\n",
    "|Total assets (USD Billion)|1767.81|1779.50|1717.53|\n",
    "|Tangible Common Equity (USD Billion)|145.47|148.21|149.95|\n",
    "|Problem Loans / Gross Loans|1.69|1.36|1.14|\n",
    "|Tangible Common Equity / Risk Weighted Assets|14.36|15.35|15.34|\n",
    "|Problem Loans / (Tangible Common Equity + Loan Loss Reserve)|9.18|7.37|6.45|\n",
    "|Net Interest Margin|2.41|2.13|2.77|\n",
    "|PPI / Average RWA|1.46|1.91|2.22|\n",
    "|Net Income / Tangible Assets|0.20|1.00|0.95|\n",
    "|Cost / Income Ratio|75.75|69.83|67.73|\n",
    "|Market Funds / Tangible Banking Assets|5.58|3.44|6.68|\n",
    "|Liquid Banking Assets / Tangible Banking Assets|41.44|41.29|35.80|\n",
    "|Gross Loans / Due to Customers|61.22|57.78|65.53|\n",
    " \n",
    "</research>\n",
    " \n",
    "<analysis>\n",
    " \n",
    "(Doc Id: PBC_1362054, Publish Date: 2023-05-11, Title:Bank of America Corporation: Update to credit analysis following ratings upgrade)\n",
    " \n",
    "1. Support and Structural Considerations:\n",
    "About Moody's Bank ScorecardOur Bank Scorecard is designed to capture, express and explain in summary form our Rating Committee's judgment. When read in conjunction with our research, a fulsome presentation of our judgment is expressed. As a result, the output of our Scorecard may materially differ from that suggested by raw data alone (though it has been calibrated to avoid the frequent need for strong divergence). The Scorecard output and the individual scores are discussed in rating committees and may be adjusted up or down to reflect conditions specific to each rated entity. As per Moody's Banks rating methodology, the historic ratios in the scorecard for Capital are as of most recent period, for Asset Risk and Profitability they are the worse of the most recent year-to-date period or the average of the last three years and the most recent year-to-date, and for Funding Structure and Liquid Resources they are as of the most recent year-end.\n",
    "2. RATINGS:\n",
    "Please see the ratings section at the end of this report for more information. The ratings and outlook shown reflect information as of the publication date. \n",
    "Summary:\n",
    "Bank of America Corporation (BAC, A1 stable) is the parent holding company for the second largest banking group in the US. The group is a global systemically important bank operating primarily through its principal bank subsidiary, Bank of America N.A. (BANA, Aa1 stable, a2 baseline credit assessment). BAC's credit profile is supported by its conservative risk appetite, a balanced and diversified business mix, strong funding and liquidity, robust cost discipline, strengthened capital and resilient profitability. During 2022, in response to increases in its regulatory capital requirements, BAC boosted its capital ratios significantly.\n",
    " \n",
    "</analysis>\n",
    " \n",
    "<questions>\n",
    "1. Can you compare the financials for JPM, Bank of America and Wells fargo in a table format?\n",
    "2. For statements referenced from documents include a reference in the format [docId].\n",
    "DO NOT UNDER ANY CIRCUMSTANCES USE ANY DATA OTHER THAN MENTIONED IN THE RESEARCH SECTION.\n",
    "</questions>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2dd11-4c82-40c2-b9d5-e24f1b7f82de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "body = json.dumps({\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": prompts_to_messages(content_prompt),\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "})\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "rprint(Markdown(response_body.get(\"content\")[0][\"text\"]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c5d1f",
   "metadata": {},
   "source": [
    "### Multi-modal prompts\n",
    "The Claude 3 family of models is not only limited to text based prompts. We can supply images as well. This is useful in a number of use cases including:\n",
    "- Extracting information from charts, graphs, diagrams, etc\n",
    "- Summarizing rich content\n",
    "- Answering questions based on images\n",
    "- Being able to effective handle more complex format types. For example we can ask Claude to narrate a PowerPoint presentation and then utilize the narrative in downstream semantic search applications\n",
    "\n",
    "Let's look at an example using a page out of JPMC's 2022 Annual Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c79b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# First we read the PDF file\n",
    "# Then we convert the PDF to an image\n",
    "\n",
    "pdf_path = \"data/jpmc_annual_report_page_6.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "img = convert_pdf_to_image(doc, page_number=0, dpi=150)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27526091",
   "metadata": {},
   "source": [
    "We will provide two prompts to the model:\n",
    "- **Text Prompt**: Provide instructions for what data to extract from the image and the desired output format\n",
    "- **Image Prompt**: Provide the image to the model as a base64 encoded string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83d0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_prompt = \"\"\"\n",
    "Extract the data contained in the chart and provide it in a json format. The data should include the Net Income, Diluted EPS, and ROTCE from 2004 to 2022 and should be structured as follows:\n",
    "{\"net_income\":{\"units:\"$B\", \"data_points\":{\"2004\": 4.5}}, \"diluted_eps\":{...}, \"rotce\":{...}}\n",
    "\n",
    "\"\"\"\n",
    "image_prompt = convert_pil_image_to_b64(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791927b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body = json.dumps(\n",
    "    {\n",
    "        \"max_tokens\": 1024,\n",
    "        \"messages\": prompts_to_messages(\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"text_prompt\": text_prompt,\n",
    "                    \"image_prompt\": image_prompt,\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"text_prompt\": \"```json\"}, # we start the generated response with a json code block to format the response\n",
    "            ]\n",
    "        ),\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    }\n",
    ")\n",
    "\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read().decode(\"utf8\"))\n",
    "\n",
    "rprint(Markdown(f'```json\\n{response_body.get(\"content\")[0][\"text\"]}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42109d",
   "metadata": {},
   "source": [
    "Are the numbers above accurate? With such a simple prompt, likely not.\n",
    "\n",
    "What can we do to make it better? \n",
    "\n",
    "See if you can adjust the prompt to get more accurate results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d69cc-7cfb-4066-9a4a-2c846a11e02c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We are just getting warmed up, we explored basic prompting techiques with Claude model. Please proceed to the next Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ddb2d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
